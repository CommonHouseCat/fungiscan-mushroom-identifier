# -*- coding: utf-8 -*-
"""resnet50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JeqX4Bt3-6ODQnqMNNU7-iC5a4M1NR1m
"""

import os
import torch
import torch.nn as nn
from torchvision import models
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
from tqdm import tqdm
from PIL import Image
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc, precision_recall_curve
from sklearn.preprocessing import label_binarize
import albumentations as A
from albumentations.pytorch import ToTensorV2
import matplotlib.pyplot as plt
import seaborn as sns
import timm
from sklearn.manifold import TSNE

"""# =============== Paths & Variables ==============="""

DATA_DIR = "/kaggle/input/mushroom2/split_data"
OUTPUT_DIR = "/kaggle/working/models"
os.makedirs(OUTPUT_DIR, exist_ok=True)
IMG_SIZE = (224, 224)
BATCH_SIZE = 64
EPOCHS = 32
NUM_WORKERS = 4

"""# =============== Augmentation Pipeline ==============="""

augmentation_pipeline = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.2),
    A.Rotate(limit=30, p=0.7),
    A.RandomBrightnessContrast(0.3, 0.3, p=0.5),
    A.HueSaturationValue(20, 30, 20, p=0.5),
    A.GaussNoise(p=0.3),
    A.GaussianBlur(blur_limit=(3, 5), p=0.2),
    A.CoarseDropout(num_holes_range=(1, 8), hole_height_range=(20, 20), hole_width_range=(20, 20), p=0.3),
    A.RandomGamma((80, 120), p=0.3),
    A.Resize(*IMG_SIZE),
    A.Normalize(),
    ToTensorV2(),
])

basic_transform = A.Compose([
    A.Resize(*IMG_SIZE),
    A.Normalize(),
    ToTensorV2(),
])

"""# ================== Dataset Class =================="""

class CSVDataset(Dataset):
    def __init__(self, csv_path, split, label_to_idx, transform=None):
        df = pd.read_csv(csv_path)
        df["filepath"] = df["filepath"].apply(lambda x: os.path.join(DATA_DIR, split, x))
        self.samples = [(row.filepath, label_to_idx[row.label])
                        for _, row in df.iterrows() if os.path.exists(row.filepath)]
        self.transform = transform

    def __len__(self): return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = np.array(Image.open(path).convert("RGB"))
        if self.transform:
            img = self.transform(image=img)["image"]
        return img, label

"""# ================== Labels & Datasets =================="""

train_df = pd.read_csv(os.path.join(DATA_DIR, "train/train.csv"))
class_names = sorted(train_df["label"].unique())
num_classes = len(class_names)
label_to_idx = {name: idx for idx, name in enumerate(class_names)}

train_ds = CSVDataset(os.path.join(DATA_DIR, "train/train.csv"), "train", label_to_idx, augmentation_pipeline)
val_ds   = CSVDataset(os.path.join(DATA_DIR, "validate/validate.csv"), "validate", label_to_idx, basic_transform)
test_ds  = CSVDataset(os.path.join(DATA_DIR, "test/test.csv"), "test", label_to_idx, basic_transform)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

"""# ================== Model Definition =================="""

class CNNOnly(nn.Module):
    def __init__(self, num_classes):
        super(CNNOnly, self).__init__()
        base_model = models.resnet50(weights="IMAGENET1K_V1")
        self.features = nn.Sequential(*list(base_model.children())[:-2])
        self.pool = nn.AdaptiveAvgPool2d((1, 1))

        with torch.no_grad():
            dummy = torch.randn(1, 3, 224, 224)
            feat_dim = self.features(dummy).shape[1]
            print(f"[INFO] CNN feature dimension (ResNet-50): {feat_dim}")

        self.classifier = nn.Sequential(
            nn.Linear(feat_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.pool(x)
        x = torch.flatten(x, 1)
        return self.classifier(x)

"""# ================== Multi-GPU Setup =================="""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNNOnly(num_classes=num_classes).to(device)
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs")
    model = nn.DataParallel(model)

"""# ================== Optimizer & Loss =================="""

criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

"""# ================== Training =================="""

train_loss_hist, val_loss_hist, train_acc_hist, val_acc_hist = [], [], [], []

for epoch in range(EPOCHS):
    model.train()
    running_loss, correct_train, total_train = 0, 0, 0

    for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        _, preds = torch.max(outputs, 1)
        correct_train += (preds == labels).sum().item()
        total_train += labels.size(0)

    scheduler.step()

    train_loss = running_loss / len(train_loader)
    train_acc = correct_train / total_train
    train_loss_hist.append(train_loss)
    train_acc_hist.append(train_acc)

    # ===== Validation =====
    model.eval()
    val_loss, correct_val, total_val = 0, 0, 0
    with torch.no_grad():
        for imgs, labels in val_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            val_loss += criterion(outputs, labels).item()
            _, preds = torch.max(outputs, 1)
            correct_val += (preds == labels).sum().item()
            total_val += labels.size(0)

    val_loss /= len(val_loader)
    val_acc = correct_val / total_val
    val_loss_hist.append(val_loss)
    val_acc_hist.append(val_acc)

    print( # Logging
        f"Epoch [{epoch+1}/{EPOCHS}] | "
        f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | "
        f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%"
    )

"""# ================== Evaluation =================="""

def evaluate(loader):
    preds, targets = [], []
    model.eval()
    with torch.no_grad():
        for imgs, labels in loader:
            imgs = imgs.to(device)
            outputs = model(imgs)
            _, pred = torch.max(outputs, 1)
            preds.extend(pred.cpu().numpy())
            targets.extend(labels.numpy())
    return np.array(targets), np.array(preds)

y_val, pred_val = evaluate(val_loader)
y_test, pred_test = evaluate(test_loader)

print("\nValidation classification report:")
print(classification_report(y_val, pred_val, target_names=class_names))
print("\nTest classification report:")
print(classification_report(y_test, pred_test, target_names=class_names))

cm = confusion_matrix(y_test, pred_test)
plt.figure(figsize=(15, 12))
sns.heatmap(cm, cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.title("ResNet50 Confusion Matrix (Test)")
plt.savefig("ResNet50_confusion_matrix.png")

plt.figure()
plt.plot(train_loss_hist, label="Train Loss")
plt.plot(val_loss_hist, label="Val Loss")
plt.legend()
plt.title("ResNet50 Loss")
plt.savefig("ResNet50_loss.png")

plt.figure()
plt.plot(train_acc_hist, label="Train Acc")
plt.plot(val_acc_hist, label="Val Acc")
plt.legend()
plt.title("ResNet50 Accuracy")
plt.savefig("ResNet50_acc.png")

# ================== EXTRA METRICS (MACRO & MICRO) ==================

# Get probabilities for metrics
model.eval()
all_probs = []
all_targets = []

with torch.no_grad():
    for imgs, labels in test_loader:
        imgs = imgs.to(device)
        outputs = model(imgs)
        probs = torch.softmax(outputs, dim=1)
        all_probs.append(probs.cpu().numpy())
        all_targets.append(labels.numpy())

all_probs = np.vstack(all_probs)
all_targets = np.hstack(all_targets)

# Binarize targets for ROC/PR
y_bin = label_binarize(all_targets, classes=np.arange(num_classes))

# ---- MICRO & MACRO 1-line metrics ----
precision_micro = precision_score(all_targets, pred_test, average="micro")
recall_micro    = recall_score(all_targets, pred_test, average="micro")
f1_micro        = f1_score(all_targets, pred_test, average="micro")

precision_macro = precision_score(all_targets, pred_test, average="macro")
recall_macro    = recall_score(all_targets, pred_test, average="macro")
f1_macro        = f1_score(all_targets, pred_test, average="macro")

print("\n=== 1-Line Summary Metrics ===")
print(f"Precision (micro): {precision_micro:.4f}")
print(f"Recall    (micro): {recall_micro:.4f}")
print(f"F1-score  (micro): {f1_micro:.4f}")
print(f"Precision (macro): {precision_macro:.4f}")
print(f"Recall    (macro): {recall_macro:.4f}")
print(f"F1-score  (macro): {f1_macro:.4f}")

# ================== MACRO ROC CURVE ==================
fpr_list = []
tpr_list = []

for i in range(num_classes):
    fpr_i, tpr_i, _ = roc_curve(y_bin[:, i], all_probs[:, i])
    fpr_list.append(fpr_i)
    tpr_list.append(tpr_i)

all_fpr = np.unique(np.concatenate(fpr_list))
mean_tpr = np.zeros_like(all_fpr)

for i in range(num_classes):
    mean_tpr += np.interp(all_fpr, fpr_list[i], tpr_list[i])

mean_tpr /= num_classes
auc_macro = auc(all_fpr, mean_tpr)

plt.figure(figsize=(8, 6))
plt.plot(all_fpr, mean_tpr, lw=2, label=f"Macro ROC (AUC={auc_macro:.3f})")
plt.plot([0, 1], [0, 1], "--")
plt.title("Macro-Averaged ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.savefig("macro_roc_curve_vit.png")

# ================== MACRO PRECISION–RECALL CURVE ==================
precision_list = []
recall_list = []

for i in range(num_classes):
    p_i, r_i, _ = precision_recall_curve(y_bin[:, i], all_probs[:, i])
    precision_list.append(p_i)
    recall_list.append(r_i)

recall_grid = np.linspace(0, 1, 500)
precision_macro_curve = np.zeros_like(recall_grid)

for p_i, r_i in zip(precision_list, recall_list):
    p_interp = np.interp(recall_grid, r_i[::-1], p_i[::-1])
    precision_macro_curve += p_interp

precision_macro_curve /= num_classes

plt.figure(figsize=(8, 6))
plt.plot(recall_grid, precision_macro_curve, lw=2, label="Macro PR Curve")
plt.title("Macro Precision–Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()
plt.savefig("macro_pr_curve_vit.png")

# ================== MACRO F1 CURVE ==================
f1_macro_curve = 2 * (precision_macro_curve * recall_grid) / \
    (precision_macro_curve + recall_grid + 1e-9)

plt.figure(figsize=(8, 6))
plt.plot(recall_grid, f1_macro_curve, lw=2, label="Macro F1 Curve")
plt.title("Macro F1 Score Curve")
plt.xlabel("Recall")
plt.ylabel("F1 Score")
plt.legend()
plt.savefig("macro_f1_curve_vit.png")

"""# ========== T-SNE VISUALIZATION =========="""

def extract_cnn_features(model, loader, device, max_samples=1500):
    model.eval()
    feats, labels = [], []
    count = 0
    with torch.no_grad():
        for imgs, lbls in loader:
            imgs = imgs.to(device)
            lbls = lbls.to(device)

            outputs = model.features(imgs) if hasattr(model, "features") else model(imgs)

            if outputs.ndim > 2:
                feats_batch = torch.mean(outputs, dim=[2, 3])
            else:
                feats_batch = outputs

            feats.append(feats_batch.cpu())
            labels.append(lbls.cpu())

            count += len(lbls)
            if count >= max_samples:
                break

    feats = torch.cat(feats, dim=0)
    labels = torch.cat(labels, dim=0)
    return feats.numpy(), labels.numpy()

features, labels = extract_cnn_features(model, train_loader, device)
subset = 1500 if len(features) > 1500 else len(features)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
embedded = tsne.fit_transform(features[:subset])

plt.figure(figsize=(10, 8))
plt.scatter(embedded[:, 0], embedded[:, 1], c=labels[:subset], cmap="tab20", alpha=0.6)
plt.title("t-SNE Visualization of ResNet50 Features (Global Avg Pooled)")
plt.savefig("ResNet50_visualization.png")
plt.show()